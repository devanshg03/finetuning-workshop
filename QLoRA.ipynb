{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mlx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mlx_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading\n",
      "Fetching 13 files:   0%|                                 | 0/13 [00:00<?, ?it/s]\n",
      "model-00002-of-00002.safetensors:   0%|             | 0.00/2.67G [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   0%|             | 0.00/4.97G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "added_tokens.json: 100%|███████████████████████| 306/306 [00:00<00:00, 1.01MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "configuration_phi3.py:   0%|                        | 0.00/11.2k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "config.json: 100%|█████████████████████████| 3.45k/3.45k [00:00<00:00, 29.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "configuration_phi3.py: 100%|███████████████| 11.2k/11.2k [00:00<00:00, 7.20MB/s]\n",
      "\n",
      "\n",
      "\n",
      "generation_config.json:   0%|                         | 0.00/195 [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "generation_config.json: 100%|███████████████████| 195/195 [00:00<00:00, 306kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Fetching 13 files:   8%|█▉                       | 1/13 [00:00<00:07,  1.54it/s]\n",
      "\n",
      "\n",
      "model.safetensors.index.json: 100%|████████| 16.3k/16.3k [00:00<00:00, 18.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "modeling_phi3.py: 100%|████████████████████| 73.8k/73.8k [00:00<00:00, 4.04MB/s]\n",
      "\n",
      "\n",
      "\n",
      "tokenizer.model:   0%|                               | 0.00/500k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "tokenizer.model: 100%|███████████████████████| 500k/500k [00:00<00:00, 4.65MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tokenizer.model: 100%|███████████████████████| 500k/500k [00:00<00:00, 4.59MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "sample_finetune.py: 100%|██████████████████| 6.13k/6.13k [00:00<00:00, 5.46MB/s]\n",
      "\n",
      "\n",
      "\n",
      "tokenizer.json:   0%|                               | 0.00/1.84M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "special_tokens_map.json: 100%|█████████████████| 665/665 [00:00<00:00, 6.21MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tokenizer_config.json: 100%|███████████████| 3.98k/3.98k [00:00<00:00, 35.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tokenizer.json: 100%|██████████████████████| 1.84M/1.84M [00:00<00:00, 4.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00002.safetensors:   0%|    | 10.5M/2.67G [00:02<10:11, 4.35MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   0%|    | 10.5M/4.97G [00:02<23:05, 3.58MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:   1%|    | 21.0M/2.67G [00:03<07:49, 5.64MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   1%|    | 31.5M/2.67G [00:05<06:51, 6.41MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   2%|    | 41.9M/2.67G [00:06<06:35, 6.64MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   2%|    | 52.4M/2.67G [00:08<06:40, 6.54MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   2%|    | 62.9M/2.67G [00:09<06:12, 7.00MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   3%|    | 73.4M/2.67G [00:10<05:44, 7.53MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   3%|▏   | 83.9M/2.67G [00:12<05:32, 7.77MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   0%|    | 21.0M/4.97G [00:12<52:49, 1.56MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:   4%|▏   | 94.4M/2.67G [00:13<05:18, 8.09MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   4%|▏    | 105M/2.67G [00:14<05:44, 7.44MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   4%|▏    | 115M/2.67G [00:17<07:05, 6.00MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   5%|▏    | 126M/2.67G [00:19<07:34, 5.60MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   5%|▎    | 136M/2.67G [00:20<06:51, 6.16MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   5%|▎    | 147M/2.67G [00:22<06:12, 6.78MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   6%|▎    | 157M/2.67G [00:23<05:42, 7.34MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   1%|  | 31.5M/4.97G [00:23<1:09:20, 1.19MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:   6%|▎    | 168M/2.67G [00:24<05:24, 7.72MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   7%|▎    | 178M/2.67G [00:25<05:24, 7.67MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   7%|▎    | 189M/2.67G [00:27<05:06, 8.09MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   7%|▎    | 199M/2.67G [00:28<04:54, 8.39MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   8%|▍    | 210M/2.67G [00:29<04:55, 8.33MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   8%|▍    | 220M/2.67G [00:30<05:06, 7.99MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   1%|  | 41.9M/4.97G [00:31<1:06:37, 1.23MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:   9%|▍    | 231M/2.67G [00:32<05:02, 8.05MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   9%|▍    | 241M/2.67G [00:33<05:10, 7.82MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   9%|▍    | 252M/2.67G [00:34<04:55, 8.17MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  10%|▍    | 262M/2.67G [00:35<04:47, 8.39MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  10%|▌    | 273M/2.67G [00:37<04:52, 8.19MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  11%|▌    | 283M/2.67G [00:38<04:37, 8.61MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   1%|  | 52.4M/4.97G [00:38<1:02:12, 1.32MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  11%|▌    | 294M/2.67G [00:39<04:33, 8.70MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  11%|▌    | 304M/2.67G [00:40<04:33, 8.66MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  12%|▌    | 315M/2.67G [00:41<04:25, 8.86MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  12%|▌    | 325M/2.67G [00:43<04:33, 8.56MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  13%|▋    | 336M/2.67G [00:44<04:29, 8.68MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  13%|▋    | 346M/2.67G [00:45<04:30, 8.59MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   1%|    | 62.9M/4.97G [00:45<59:41, 1.37MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  13%|▋    | 357M/2.67G [00:46<04:36, 8.36MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  14%|▋    | 367M/2.67G [00:48<04:56, 7.78MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  14%|▋    | 377M/2.67G [00:50<05:33, 6.87MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   1%|    | 73.4M/4.97G [00:51<56:08, 1.45MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  15%|▋    | 388M/2.67G [00:52<06:00, 6.33MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  15%|▋    | 398M/2.67G [00:54<06:34, 5.76MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  15%|▊    | 409M/2.67G [00:56<07:01, 5.36MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   2%|    | 83.9M/4.97G [00:57<51:18, 1.59MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  16%|▊    | 419M/2.67G [01:00<08:32, 4.39MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   2%|    | 94.4M/4.97G [01:04<52:22, 1.55MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  16%|▊    | 430M/2.67G [01:07<13:57, 2.68MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   2%|     | 105M/4.97G [01:12<54:39, 1.48MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  16%|▊    | 440M/2.67G [01:17<19:34, 1.90MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   2%|     | 115M/4.97G [01:21<59:50, 1.35MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  17%|▊    | 451M/2.67G [01:25<22:21, 1.65MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   3%|▏    | 126M/4.97G [01:27<56:05, 1.44MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  17%|▊    | 461M/2.67G [01:31<22:04, 1.67MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   3%|▏    | 136M/4.97G [01:32<51:14, 1.57MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  18%|▉    | 472M/2.67G [01:36<21:05, 1.74MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   3%|▏    | 147M/4.97G [01:37<47:19, 1.70MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   3%|▏    | 157M/4.97G [01:43<45:03, 1.78MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  18%|▉    | 482M/2.67G [01:43<21:47, 1.67MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  18%|▉    | 493M/2.67G [01:48<19:54, 1.82MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   3%|▏    | 168M/4.97G [01:48<43:17, 1.85MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  19%|▉    | 503M/2.67G [01:53<18:52, 1.91MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   4%|▏    | 178M/4.97G [01:54<44:28, 1.80MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  19%|▉    | 514M/2.67G [01:58<19:10, 1.87MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   4%|▏    | 189M/4.97G [02:00<44:24, 1.80MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  20%|▉    | 524M/2.67G [02:04<19:20, 1.85MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   4%|▏    | 199M/4.97G [02:05<42:56, 1.85MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  20%|█    | 535M/2.67G [02:10<19:17, 1.84MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   4%|▏    | 210M/4.97G [02:11<42:58, 1.85MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  20%|█    | 545M/2.67G [02:15<18:58, 1.87MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   4%|▏    | 220M/4.97G [02:16<42:01, 1.88MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   5%|▏    | 231M/4.97G [02:21<39:49, 1.98MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  21%|█    | 556M/2.67G [02:21<18:48, 1.87MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   5%|▏    | 241M/4.97G [02:25<37:41, 2.09MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  21%|█    | 566M/2.67G [02:25<17:27, 2.01MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   5%|▎    | 252M/4.97G [02:30<36:13, 2.17MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  22%|█    | 577M/2.67G [02:31<18:03, 1.93MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   5%|▎    | 262M/4.97G [02:39<45:46, 1.71MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  22%|█    | 587M/2.67G [02:41<22:28, 1.54MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   5%|▎    | 273M/4.97G [02:46<48:56, 1.60MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  22%|█    | 598M/2.67G [02:51<25:19, 1.36MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   6%|▏  | 283M/4.97G [02:59<1:02:24, 1.25MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  23%|█▏   | 608M/2.67G [03:02<28:41, 1.20MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   6%|▏  | 294M/4.97G [03:07<1:02:45, 1.24MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  23%|█▏   | 619M/2.67G [03:11<28:53, 1.18MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   6%|▎    | 304M/4.97G [03:14<58:01, 1.34MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  24%|█▏   | 629M/2.67G [03:22<29:58, 1.13MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   6%|▏  | 315M/4.97G [03:24<1:03:48, 1.22MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  24%|█▏   | 640M/2.67G [03:32<30:50, 1.10MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   7%|▏  | 325M/4.97G [03:32<1:01:13, 1.27MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   7%|▎    | 336M/4.97G [03:38<57:23, 1.35MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  24%|█▏   | 650M/2.67G [03:39<28:22, 1.19MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  25%|█▏   | 661M/2.67G [03:44<24:43, 1.35MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   7%|▎    | 346M/4.97G [03:44<53:25, 1.44MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  25%|█▎   | 671M/2.67G [03:49<21:49, 1.53MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   7%|▎    | 357M/4.97G [03:50<49:45, 1.55MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  26%|█▎   | 682M/2.67G [03:54<19:39, 1.69MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   7%|▎    | 367M/4.97G [03:55<44:32, 1.72MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  26%|█▎   | 692M/2.67G [03:58<17:31, 1.88MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   8%|▍    | 377M/4.97G [03:59<40:26, 1.89MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  26%|█▎   | 703M/2.67G [04:02<15:45, 2.08MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   8%|▍    | 388M/4.97G [04:04<39:31, 1.93MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  27%|█▎   | 713M/2.67G [04:05<14:26, 2.26MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   8%|▍    | 398M/4.97G [04:09<37:41, 2.02MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  27%|█▎   | 724M/2.67G [04:09<13:14, 2.45MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  27%|█▎   | 734M/2.67G [04:12<12:06, 2.66MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   8%|▍    | 409M/4.97G [04:13<35:43, 2.13MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  28%|█▍   | 744M/2.67G [04:15<11:40, 2.75MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   8%|▍    | 419M/4.97G [04:20<39:33, 1.92MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  28%|█▍   | 755M/2.67G [04:21<13:06, 2.44MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   9%|▍    | 430M/4.97G [04:24<36:24, 2.08MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  29%|█▍   | 765M/2.67G [04:24<11:58, 2.65MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  29%|█▍   | 776M/2.67G [04:27<11:29, 2.75MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   9%|▍    | 440M/4.97G [04:29<35:44, 2.11MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  29%|█▍   | 786M/2.67G [04:31<11:27, 2.74MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  30%|█▍   | 797M/2.67G [04:36<12:20, 2.53MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   9%|▍    | 451M/4.97G [04:37<42:15, 1.78MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  30%|█▌   | 807M/2.67G [04:43<14:40, 2.12MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   9%|▍    | 461M/4.97G [04:44<45:02, 1.67MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  31%|█▌   | 818M/2.67G [04:49<15:08, 2.04MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:   9%|▍    | 472M/4.97G [04:50<45:12, 1.66MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  10%|▍    | 482M/4.97G [04:55<42:40, 1.75MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  31%|█▌   | 828M/2.67G [04:55<16:27, 1.86MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  10%|▍    | 493M/4.97G [05:01<42:44, 1.75MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  31%|█▌   | 839M/2.67G [05:03<18:07, 1.68MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  10%|▌    | 503M/4.97G [05:07<42:07, 1.77MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  32%|█▌   | 849M/2.67G [05:10<18:16, 1.66MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  10%|▌    | 514M/4.97G [05:13<41:17, 1.80MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  32%|█▌   | 860M/2.67G [05:16<18:36, 1.62MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  11%|▌    | 524M/4.97G [05:19<42:43, 1.73MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  33%|█▋   | 870M/2.67G [05:21<16:47, 1.79MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  11%|▌    | 535M/4.97G [05:25<42:23, 1.74MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  33%|█▋   | 881M/2.67G [05:26<16:04, 1.86MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  33%|█▋   | 891M/2.67G [05:31<15:17, 1.94MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  11%|▌    | 545M/4.97G [05:31<42:12, 1.75MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  34%|█▋   | 902M/2.67G [05:35<14:16, 2.07MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  11%|▌    | 556M/4.97G [05:36<38:36, 1.91MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  11%|▌    | 566M/4.97G [05:40<35:57, 2.04MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  34%|█▋   | 912M/2.67G [05:40<14:11, 2.06MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  12%|▌    | 577M/4.97G [05:44<34:12, 2.14MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  35%|█▋   | 923M/2.67G [05:46<14:54, 1.95MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  12%|▌    | 587M/4.97G [05:48<32:48, 2.23MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  12%|▌    | 598M/4.97G [05:53<31:33, 2.31MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  35%|█▋   | 933M/2.67G [05:53<16:03, 1.80MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  12%|▌    | 608M/4.97G [05:56<29:34, 2.46MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  35%|█▊   | 944M/2.67G [05:59<15:53, 1.81MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  12%|▌    | 619M/4.97G [06:00<27:45, 2.61MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  13%|▋    | 629M/4.97G [06:03<26:45, 2.70MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  36%|█▊   | 954M/2.67G [06:05<15:58, 1.79MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  13%|▋    | 640M/4.97G [06:08<28:19, 2.55MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  36%|█▊   | 965M/2.67G [06:10<15:02, 1.89MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  13%|▋    | 650M/4.97G [06:13<30:57, 2.33MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  37%|█▊   | 975M/2.67G [06:15<14:34, 1.94MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  13%|▋    | 661M/4.97G [06:20<34:57, 2.06MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  37%|█▊   | 986M/2.67G [06:20<14:11, 1.98MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  13%|▋    | 671M/4.97G [06:25<35:09, 2.04MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  37%|█▊   | 996M/2.67G [06:26<14:48, 1.88MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  14%|▋    | 682M/4.97G [06:30<35:04, 2.04MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  38%|█▌  | 1.01G/2.67G [06:31<14:31, 1.91MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  38%|█▌  | 1.02G/2.67G [06:37<14:15, 1.93MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  14%|▋    | 692M/4.97G [06:37<39:08, 1.82MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  38%|█▌  | 1.03G/2.67G [06:41<13:25, 2.04MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  14%|▋    | 703M/4.97G [06:43<39:13, 1.81MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  39%|█▌  | 1.04G/2.67G [06:46<13:00, 2.09MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  14%|▋    | 713M/4.97G [06:49<39:18, 1.81MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  39%|█▌  | 1.05G/2.67G [06:52<14:02, 1.92MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  14%|▋    | 713M/4.97G [06:59<39:18, 1.81MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  15%|▋    | 724M/4.97G [06:59<48:16, 1.47MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  40%|█▌  | 1.06G/2.67G [07:00<15:49, 1.70MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  15%|▋    | 734M/4.97G [07:06<48:11, 1.47MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  40%|█▌  | 1.07G/2.67G [07:07<16:12, 1.65MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  15%|▋    | 744M/4.97G [07:13<47:18, 1.49MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  40%|█▌  | 1.08G/2.67G [07:16<17:54, 1.48MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  15%|▊    | 755M/4.97G [07:21<47:41, 1.47MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  41%|█▋  | 1.09G/2.67G [07:24<18:53, 1.39MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  15%|▊    | 765M/4.97G [07:27<46:36, 1.50MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  41%|█▋  | 1.10G/2.67G [07:30<17:31, 1.49MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  16%|▊    | 776M/4.97G [07:34<45:56, 1.52MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  42%|█▋  | 1.11G/2.67G [07:35<16:04, 1.61MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  16%|▊    | 786M/4.97G [07:41<46:05, 1.51MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  42%|█▋  | 1.12G/2.67G [07:41<15:27, 1.67MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  16%|▊    | 797M/4.97G [07:46<43:01, 1.62MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  42%|█▋  | 1.13G/2.67G [07:47<14:55, 1.72MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  16%|▊    | 807M/4.97G [07:52<40:18, 1.72MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  43%|█▋  | 1.14G/2.67G [07:52<13:49, 1.84MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  16%|▊    | 818M/4.97G [07:56<37:01, 1.87MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  43%|█▋  | 1.15G/2.67G [07:57<13:23, 1.89MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  17%|▊    | 828M/4.97G [08:01<36:07, 1.91MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  44%|█▋  | 1.16G/2.67G [08:02<13:15, 1.89MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  17%|▊    | 839M/4.97G [08:05<33:21, 2.06MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  44%|█▊  | 1.17G/2.67G [08:08<13:01, 1.91MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  17%|▊    | 849M/4.97G [08:10<31:36, 2.17MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  44%|█▊  | 1.18G/2.67G [08:13<12:27, 1.99MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  17%|▊    | 860M/4.97G [08:14<30:54, 2.22MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  18%|▉    | 870M/4.97G [08:19<30:40, 2.23MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  45%|█▊  | 1.20G/2.67G [08:19<13:28, 1.82MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  18%|▉    | 881M/4.97G [08:22<28:44, 2.37MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  45%|█▊  | 1.21G/2.67G [08:26<14:12, 1.72MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  18%|▉    | 891M/4.97G [08:27<28:21, 2.40MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  46%|█▊  | 1.22G/2.67G [08:32<13:31, 1.79MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  18%|▉    | 902M/4.97G [08:33<32:45, 2.07MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  46%|█▊  | 1.23G/2.67G [08:37<12:52, 1.87MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  18%|▉    | 912M/4.97G [08:38<32:43, 2.07MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  46%|█▊  | 1.24G/2.67G [08:42<12:47, 1.87MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  19%|▉    | 923M/4.97G [08:44<33:36, 2.01MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  47%|█▊  | 1.25G/2.67G [08:48<12:45, 1.86MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  19%|▉    | 933M/4.97G [08:50<34:24, 1.96MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  47%|█▉  | 1.26G/2.67G [08:56<14:12, 1.65MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  19%|▉    | 944M/4.97G [08:58<39:02, 1.72MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  48%|█▉  | 1.27G/2.67G [09:04<15:29, 1.51MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  19%|▉    | 954M/4.97G [09:05<42:20, 1.58MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  48%|█▉  | 1.28G/2.67G [09:12<15:34, 1.49MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  19%|▉    | 965M/4.97G [09:12<43:02, 1.55MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  20%|▉    | 975M/4.97G [09:18<40:29, 1.65MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  48%|█▉  | 1.29G/2.67G [09:19<15:25, 1.49MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  20%|▉    | 986M/4.97G [09:24<39:15, 1.69MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  49%|█▉  | 1.30G/2.67G [09:26<15:23, 1.48MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  49%|█▉  | 1.31G/2.67G [09:31<13:53, 1.63MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  20%|█    | 996M/4.97G [09:31<41:07, 1.61MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  49%|█▉  | 1.32G/2.67G [09:37<13:34, 1.66MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  20%|▊   | 1.01G/4.97G [09:41<47:12, 1.40MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  50%|█▉  | 1.33G/2.67G [09:44<14:06, 1.58MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  20%|▊   | 1.02G/4.97G [09:49<47:51, 1.38MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  50%|██  | 1.34G/2.67G [09:51<14:22, 1.54MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  21%|▊   | 1.03G/4.97G [09:56<46:34, 1.41MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  51%|██  | 1.35G/2.67G [09:59<14:52, 1.47MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  21%|▊   | 1.04G/4.97G [10:04<47:31, 1.38MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  51%|██  | 1.36G/2.67G [10:06<14:32, 1.50MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  21%|▊   | 1.05G/4.97G [10:10<45:38, 1.43MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  51%|██  | 1.37G/2.67G [10:13<14:34, 1.48MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  21%|▊   | 1.06G/4.97G [10:16<42:00, 1.55MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  52%|██  | 1.38G/2.67G [10:19<13:39, 1.57MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  22%|▊   | 1.07G/4.97G [10:21<39:23, 1.65MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  52%|██  | 1.39G/2.67G [10:24<12:20, 1.72MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  22%|▊   | 1.08G/4.97G [10:28<40:46, 1.59MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  53%|██  | 1.41G/2.67G [10:29<11:37, 1.81MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  53%|██  | 1.42G/2.67G [10:33<10:45, 1.94MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  22%|▉   | 1.09G/4.97G [10:35<40:49, 1.58MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  53%|██▏ | 1.43G/2.67G [10:40<11:43, 1.77MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  22%|▉   | 1.10G/4.97G [10:41<39:15, 1.64MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  22%|▉   | 1.11G/4.97G [10:45<35:19, 1.82MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  54%|██▏ | 1.44G/2.67G [10:47<11:57, 1.72MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  23%|▉   | 1.12G/4.97G [10:49<30:59, 2.07MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  54%|██▏ | 1.45G/2.67G [10:52<11:05, 1.84MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  23%|▉   | 1.13G/4.97G [10:53<29:17, 2.19MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  55%|██▏ | 1.46G/2.67G [10:57<10:39, 1.90MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  23%|▉   | 1.14G/4.97G [10:59<30:59, 2.06MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  55%|██▏ | 1.47G/2.67G [11:04<11:16, 1.78MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  23%|▉   | 1.15G/4.97G [11:04<31:25, 2.03MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  23%|▉   | 1.16G/4.97G [11:11<34:06, 1.86MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  55%|██▏ | 1.48G/2.67G [11:11<12:16, 1.62MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  24%|▉   | 1.17G/4.97G [11:17<34:40, 1.83MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  56%|██▏ | 1.49G/2.67G [11:17<11:49, 1.66MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  24%|▉   | 1.18G/4.97G [11:22<33:06, 1.91MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  56%|██▏ | 1.50G/2.67G [11:22<11:06, 1.76MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  24%|▉   | 1.20G/4.97G [11:28<33:59, 1.85MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  57%|██▎ | 1.51G/2.67G [11:31<12:10, 1.59MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  24%|▉   | 1.21G/4.97G [11:34<34:36, 1.81MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  57%|██▎ | 1.52G/2.67G [11:37<11:54, 1.61MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  24%|▉   | 1.22G/4.97G [11:38<32:35, 1.92MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  57%|██▎ | 1.53G/2.67G [11:43<11:32, 1.64MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  25%|▉   | 1.23G/4.97G [11:43<31:42, 1.97MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  25%|▉   | 1.24G/4.97G [11:49<31:31, 1.97MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  58%|██▎ | 1.54G/2.67G [11:49<11:08, 1.69MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  58%|██▎ | 1.55G/2.67G [11:54<10:23, 1.79MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  25%|█   | 1.25G/4.97G [11:55<33:53, 1.83MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  59%|██▎ | 1.56G/2.67G [11:59<10:03, 1.83MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  25%|█   | 1.26G/4.97G [12:02<36:12, 1.71MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  59%|██▎ | 1.57G/2.67G [12:04<09:45, 1.87MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  59%|██▎ | 1.58G/2.67G [12:10<09:49, 1.84MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  26%|█   | 1.27G/4.97G [12:11<41:17, 1.49MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  60%|██▍ | 1.59G/2.67G [12:16<09:26, 1.90MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  26%|█   | 1.28G/4.97G [12:17<39:01, 1.58MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  60%|██▍ | 1.60G/2.67G [12:21<09:05, 1.95MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  26%|█   | 1.29G/4.97G [12:23<37:07, 1.65MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  60%|██▍ | 1.61G/2.67G [12:25<08:34, 2.05MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  26%|█   | 1.30G/4.97G [12:27<33:29, 1.83MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  61%|██▍ | 1.63G/2.67G [12:29<07:55, 2.20MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  26%|█   | 1.31G/4.97G [12:31<30:32, 2.00MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  61%|██▍ | 1.64G/2.67G [12:33<07:26, 2.31MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  27%|█   | 1.32G/4.97G [12:37<31:09, 1.95MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  62%|██▍ | 1.65G/2.67G [12:38<07:43, 2.21MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  62%|██▍ | 1.66G/2.67G [12:43<07:30, 2.25MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  27%|█   | 1.33G/4.97G [12:44<33:25, 1.82MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  62%|██▍ | 1.67G/2.67G [12:49<07:59, 2.09MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  27%|█   | 1.34G/4.97G [12:51<36:31, 1.66MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  63%|██▌ | 1.68G/2.67G [12:56<09:16, 1.78MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  27%|█   | 1.35G/4.97G [13:02<43:26, 1.39MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  63%|██▌ | 1.69G/2.67G [13:04<09:52, 1.66MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  27%|█   | 1.36G/4.97G [13:09<43:16, 1.39MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  64%|██▌ | 1.70G/2.67G [13:10<09:49, 1.65MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  28%|█   | 1.37G/4.97G [13:15<40:42, 1.47MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  64%|██▌ | 1.71G/2.67G [13:19<10:56, 1.46MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  28%|█   | 1.38G/4.97G [13:23<41:25, 1.44MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  64%|██▌ | 1.72G/2.67G [13:27<11:00, 1.44MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  28%|█   | 1.39G/4.97G [13:29<39:58, 1.49MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  65%|██▌ | 1.73G/2.67G [13:34<10:39, 1.47MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  28%|█▏  | 1.41G/4.97G [13:34<35:14, 1.69MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  28%|█▏  | 1.42G/4.97G [13:38<31:53, 1.86MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  65%|██▌ | 1.74G/2.67G [13:40<10:03, 1.54MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  29%|█▏  | 1.43G/4.97G [13:44<31:33, 1.87MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  66%|██▌ | 1.75G/2.67G [13:46<09:31, 1.61MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  29%|█▏  | 1.44G/4.97G [13:51<34:23, 1.71MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  66%|██▋ | 1.76G/2.67G [13:54<10:20, 1.46MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  29%|█▏  | 1.45G/4.97G [13:58<36:39, 1.60MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  66%|██▋ | 1.77G/2.67G [14:03<11:02, 1.35MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  29%|█▏  | 1.46G/4.97G [14:06<37:43, 1.55MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  67%|██▋ | 1.78G/2.67G [14:12<11:20, 1.30MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  30%|█▏  | 1.47G/4.97G [14:13<39:05, 1.49MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  67%|██▋ | 1.79G/2.67G [14:19<10:34, 1.38MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  30%|█▏  | 1.48G/4.97G [14:20<38:21, 1.52MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  68%|██▋ | 1.80G/2.67G [14:24<09:29, 1.52MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  30%|█▏  | 1.49G/4.97G [14:26<37:23, 1.55MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  68%|██▋ | 1.81G/2.67G [14:29<08:28, 1.68MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  30%|█▏  | 1.50G/4.97G [14:32<35:18, 1.64MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  68%|██▋ | 1.82G/2.67G [14:33<07:38, 1.84MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  69%|██▋ | 1.84G/2.67G [14:38<07:12, 1.93MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  30%|█▏  | 1.51G/4.97G [14:39<35:45, 1.61MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  31%|█▏  | 1.52G/4.97G [14:44<34:05, 1.69MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  69%|██▊ | 1.85G/2.67G [14:45<07:44, 1.77MB/s]\u001b[A\n",
      "\n",
      "model-00001-of-00002.safetensors:  31%|█▏  | 1.53G/4.97G [14:49<32:22, 1.77MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00002.safetensors:  70%|██▊ | 1.86G/2.67G [14:50<07:31, 1.80MB/s]\u001b[A^C\n",
      "Fetching 13 files:  31%|███████▍                | 4/13 [14:53<33:31, 223.46s/it]\n"
     ]
    }
   ],
   "source": [
    "# Create a quantized model\n",
    "! mlx_lm.convert --hf-path microsoft/Phi-3.5-mini-instruct \\\n",
    "    --mlx-path ./Phi-3.5-mini-instruct-4bit \\\n",
    "    -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n",
      "Loading datasets\n",
      "Training\n",
      "Trainable parameters: 0.082% (3.146M/3821.080M)\n",
      "Starting training..., iters: 1000\n",
      "Iter 1: Val loss 8.065, Val took 6.661s\n",
      "Iter 10: Train loss 6.295, Learning Rate 1.000e-05, It/sec 1.861, Tokens/sec 168.575, Trained Tokens 906, Peak mem 2.655 GB\n",
      "Iter 20: Train loss 4.109, Learning Rate 1.000e-05, It/sec 1.653, Tokens/sec 170.597, Trained Tokens 1938, Peak mem 2.981 GB\n",
      "Iter 30: Train loss 3.057, Learning Rate 1.000e-05, It/sec 1.781, Tokens/sec 173.666, Trained Tokens 2913, Peak mem 2.981 GB\n",
      "Iter 40: Train loss 2.345, Learning Rate 1.000e-05, It/sec 2.066, Tokens/sec 169.827, Trained Tokens 3735, Peak mem 2.981 GB\n",
      "Iter 50: Train loss 2.168, Learning Rate 1.000e-05, It/sec 1.588, Tokens/sec 168.521, Trained Tokens 4796, Peak mem 3.441 GB\n",
      "Iter 60: Train loss 2.203, Learning Rate 1.000e-05, It/sec 1.770, Tokens/sec 176.479, Trained Tokens 5793, Peak mem 3.441 GB\n",
      "Iter 70: Train loss 1.799, Learning Rate 1.000e-05, It/sec 2.041, Tokens/sec 165.547, Trained Tokens 6604, Peak mem 3.441 GB\n",
      "Iter 80: Train loss 2.092, Learning Rate 1.000e-05, It/sec 1.744, Tokens/sec 178.397, Trained Tokens 7627, Peak mem 3.441 GB\n",
      "Iter 90: Train loss 1.971, Learning Rate 1.000e-05, It/sec 1.735, Tokens/sec 171.463, Trained Tokens 8615, Peak mem 3.441 GB\n",
      "Iter 100: Train loss 1.881, Learning Rate 1.000e-05, It/sec 1.980, Tokens/sec 174.797, Trained Tokens 9498, Peak mem 3.441 GB\n",
      "Iter 100: Saved adapter weights to adapters/adapters.safetensors and adapters/0000100_adapters.safetensors.\n",
      "Iter 110: Train loss 1.964, Learning Rate 1.000e-05, It/sec 1.532, Tokens/sec 173.983, Trained Tokens 10634, Peak mem 3.441 GB\n",
      "Iter 120: Train loss 2.137, Learning Rate 1.000e-05, It/sec 1.322, Tokens/sec 161.494, Trained Tokens 11856, Peak mem 3.658 GB\n",
      "Iter 130: Train loss 1.908, Learning Rate 1.000e-05, It/sec 1.773, Tokens/sec 173.967, Trained Tokens 12837, Peak mem 3.658 GB\n",
      "Iter 140: Train loss 1.774, Learning Rate 1.000e-05, It/sec 1.994, Tokens/sec 172.460, Trained Tokens 13702, Peak mem 3.658 GB\n",
      "Iter 150: Train loss 1.819, Learning Rate 1.000e-05, It/sec 1.570, Tokens/sec 160.103, Trained Tokens 14722, Peak mem 3.658 GB\n",
      "Iter 160: Train loss 2.103, Learning Rate 1.000e-05, It/sec 1.573, Tokens/sec 177.387, Trained Tokens 15850, Peak mem 3.658 GB\n",
      "Iter 170: Train loss 2.289, Learning Rate 1.000e-05, It/sec 1.261, Tokens/sec 150.827, Trained Tokens 17046, Peak mem 3.658 GB\n",
      "Iter 180: Train loss 1.989, Learning Rate 1.000e-05, It/sec 1.705, Tokens/sec 177.639, Trained Tokens 18088, Peak mem 3.658 GB\n",
      "Iter 190: Train loss 1.947, Learning Rate 1.000e-05, It/sec 1.778, Tokens/sec 169.266, Trained Tokens 19040, Peak mem 3.658 GB\n",
      "Iter 200: Val loss 1.936, Val took 6.479s\n",
      "Iter 200: Train loss 2.034, Learning Rate 1.000e-05, It/sec 8.934, Tokens/sec 996.985, Trained Tokens 20156, Peak mem 3.658 GB\n",
      "Iter 200: Saved adapter weights to adapters/adapters.safetensors and adapters/0000200_adapters.safetensors.\n",
      "Iter 210: Train loss 1.630, Learning Rate 1.000e-05, It/sec 2.033, Tokens/sec 169.757, Trained Tokens 20991, Peak mem 3.658 GB\n",
      "Iter 220: Train loss 1.949, Learning Rate 1.000e-05, It/sec 1.753, Tokens/sec 176.708, Trained Tokens 21999, Peak mem 3.658 GB\n",
      "Iter 230: Train loss 1.866, Learning Rate 1.000e-05, It/sec 1.612, Tokens/sec 159.624, Trained Tokens 22989, Peak mem 3.658 GB\n",
      "Iter 240: Train loss 1.958, Learning Rate 1.000e-05, It/sec 1.465, Tokens/sec 152.844, Trained Tokens 24032, Peak mem 3.658 GB\n",
      "Iter 250: Train loss 1.713, Learning Rate 1.000e-05, It/sec 1.954, Tokens/sec 165.480, Trained Tokens 24879, Peak mem 3.658 GB\n",
      "Iter 260: Train loss 1.999, Learning Rate 1.000e-05, It/sec 1.538, Tokens/sec 168.856, Trained Tokens 25977, Peak mem 3.658 GB\n",
      "Iter 270: Train loss 2.042, Learning Rate 1.000e-05, It/sec 1.429, Tokens/sec 175.296, Trained Tokens 27204, Peak mem 3.658 GB\n",
      "Iter 280: Train loss 2.038, Learning Rate 1.000e-05, It/sec 1.633, Tokens/sec 176.086, Trained Tokens 28282, Peak mem 3.658 GB\n",
      "Iter 290: Train loss 1.837, Learning Rate 1.000e-05, It/sec 1.753, Tokens/sec 177.595, Trained Tokens 29295, Peak mem 3.658 GB\n",
      "Iter 300: Train loss 1.878, Learning Rate 1.000e-05, It/sec 1.116, Tokens/sec 142.564, Trained Tokens 30572, Peak mem 4.032 GB\n",
      "Iter 300: Saved adapter weights to adapters/adapters.safetensors and adapters/0000300_adapters.safetensors.\n",
      "Iter 310: Train loss 1.843, Learning Rate 1.000e-05, It/sec 1.598, Tokens/sec 147.955, Trained Tokens 31498, Peak mem 4.032 GB\n",
      "Iter 320: Train loss 1.844, Learning Rate 1.000e-05, It/sec 1.746, Tokens/sec 170.904, Trained Tokens 32477, Peak mem 4.032 GB\n",
      "Iter 330: Train loss 1.823, Learning Rate 1.000e-05, It/sec 1.790, Tokens/sec 176.286, Trained Tokens 33462, Peak mem 4.032 GB\n",
      "Iter 340: Train loss 2.081, Learning Rate 1.000e-05, It/sec 1.637, Tokens/sec 177.639, Trained Tokens 34547, Peak mem 4.032 GB\n",
      "Iter 350: Train loss 1.881, Learning Rate 1.000e-05, It/sec 1.628, Tokens/sec 176.976, Trained Tokens 35634, Peak mem 4.032 GB\n",
      "Iter 360: Train loss 1.633, Learning Rate 1.000e-05, It/sec 1.782, Tokens/sec 168.193, Trained Tokens 36578, Peak mem 4.032 GB\n",
      "Iter 370: Train loss 1.770, Learning Rate 1.000e-05, It/sec 1.874, Tokens/sec 174.053, Trained Tokens 37507, Peak mem 4.032 GB\n",
      "Iter 380: Train loss 2.109, Learning Rate 1.000e-05, It/sec 1.704, Tokens/sec 183.325, Trained Tokens 38583, Peak mem 4.032 GB\n",
      "Iter 390: Train loss 1.986, Learning Rate 1.000e-05, It/sec 1.881, Tokens/sec 178.364, Trained Tokens 39531, Peak mem 4.032 GB\n",
      "Iter 400: Val loss 1.907, Val took 6.414s\n",
      "Iter 400: Train loss 1.963, Learning Rate 1.000e-05, It/sec 9.360, Tokens/sec 1022.138, Trained Tokens 40623, Peak mem 4.032 GB\n",
      "Iter 400: Saved adapter weights to adapters/adapters.safetensors and adapters/0000400_adapters.safetensors.\n",
      "Iter 410: Train loss 1.882, Learning Rate 1.000e-05, It/sec 1.834, Tokens/sec 178.284, Trained Tokens 41595, Peak mem 4.032 GB\n",
      "Iter 420: Train loss 1.695, Learning Rate 1.000e-05, It/sec 1.872, Tokens/sec 165.478, Trained Tokens 42479, Peak mem 4.032 GB\n",
      "Iter 430: Train loss 1.761, Learning Rate 1.000e-05, It/sec 1.902, Tokens/sec 171.179, Trained Tokens 43379, Peak mem 4.032 GB\n",
      "Iter 440: Train loss 1.945, Learning Rate 1.000e-05, It/sec 1.079, Tokens/sec 102.603, Trained Tokens 44330, Peak mem 4.032 GB\n",
      "Iter 450: Train loss 1.602, Learning Rate 1.000e-05, It/sec 2.040, Tokens/sec 171.361, Trained Tokens 45170, Peak mem 4.032 GB\n",
      "Iter 460: Train loss 1.767, Learning Rate 1.000e-05, It/sec 1.685, Tokens/sec 162.972, Trained Tokens 46137, Peak mem 4.032 GB\n",
      "Iter 470: Train loss 1.827, Learning Rate 1.000e-05, It/sec 1.799, Tokens/sec 176.676, Trained Tokens 47119, Peak mem 4.032 GB\n",
      "Iter 480: Train loss 1.897, Learning Rate 1.000e-05, It/sec 1.848, Tokens/sec 182.207, Trained Tokens 48105, Peak mem 4.032 GB\n",
      "Iter 490: Train loss 1.707, Learning Rate 1.000e-05, It/sec 1.760, Tokens/sec 174.405, Trained Tokens 49096, Peak mem 4.032 GB\n",
      "Iter 500: Train loss 1.748, Learning Rate 1.000e-05, It/sec 1.822, Tokens/sec 169.229, Trained Tokens 50025, Peak mem 4.032 GB\n",
      "Iter 500: Saved adapter weights to adapters/adapters.safetensors and adapters/0000500_adapters.safetensors.\n",
      "Iter 510: Train loss 1.984, Learning Rate 1.000e-05, It/sec 1.581, Tokens/sec 173.555, Trained Tokens 51123, Peak mem 4.032 GB\n",
      "Iter 520: Train loss 1.826, Learning Rate 1.000e-05, It/sec 1.203, Tokens/sec 120.920, Trained Tokens 52128, Peak mem 4.032 GB\n",
      "Iter 530: Train loss 1.653, Learning Rate 1.000e-05, It/sec 1.850, Tokens/sec 173.363, Trained Tokens 53065, Peak mem 4.032 GB\n",
      "Iter 540: Train loss 1.850, Learning Rate 1.000e-05, It/sec 1.427, Tokens/sec 149.384, Trained Tokens 54112, Peak mem 4.032 GB\n",
      "Iter 550: Train loss 1.998, Learning Rate 1.000e-05, It/sec 1.410, Tokens/sec 160.265, Trained Tokens 55249, Peak mem 4.032 GB\n",
      "Iter 560: Train loss 1.827, Learning Rate 1.000e-05, It/sec 1.605, Tokens/sec 161.589, Trained Tokens 56256, Peak mem 4.032 GB\n",
      "Iter 570: Train loss 1.808, Learning Rate 1.000e-05, It/sec 1.847, Tokens/sec 175.077, Trained Tokens 57204, Peak mem 4.032 GB\n",
      "Iter 580: Train loss 1.789, Learning Rate 1.000e-05, It/sec 1.906, Tokens/sec 172.695, Trained Tokens 58110, Peak mem 4.032 GB\n",
      "Iter 590: Train loss 1.915, Learning Rate 1.000e-05, It/sec 1.483, Tokens/sec 171.137, Trained Tokens 59264, Peak mem 4.032 GB\n",
      "Iter 600: Val loss 2.080, Val took 7.610s\n",
      "Iter 600: Train loss 1.621, Learning Rate 1.000e-05, It/sec 20.961, Tokens/sec 1695.709, Trained Tokens 60073, Peak mem 4.032 GB\n",
      "Iter 600: Saved adapter weights to adapters/adapters.safetensors and adapters/0000600_adapters.safetensors.\n",
      "Iter 610: Train loss 1.791, Learning Rate 1.000e-05, It/sec 1.827, Tokens/sec 162.635, Trained Tokens 60963, Peak mem 4.032 GB\n",
      "Iter 620: Train loss 1.648, Learning Rate 1.000e-05, It/sec 1.916, Tokens/sec 162.491, Trained Tokens 61811, Peak mem 4.032 GB\n",
      "Iter 630: Train loss 1.976, Learning Rate 1.000e-05, It/sec 1.540, Tokens/sec 171.565, Trained Tokens 62925, Peak mem 4.032 GB\n",
      "Iter 640: Train loss 1.826, Learning Rate 1.000e-05, It/sec 1.746, Tokens/sec 175.450, Trained Tokens 63930, Peak mem 4.032 GB\n",
      "Iter 650: Train loss 1.678, Learning Rate 1.000e-05, It/sec 1.766, Tokens/sec 168.313, Trained Tokens 64883, Peak mem 4.032 GB\n",
      "Iter 660: Train loss 2.091, Learning Rate 1.000e-05, It/sec 1.481, Tokens/sec 169.561, Trained Tokens 66028, Peak mem 4.032 GB\n",
      "Iter 670: Train loss 1.775, Learning Rate 1.000e-05, It/sec 1.328, Tokens/sec 120.804, Trained Tokens 66938, Peak mem 4.032 GB\n",
      "Iter 680: Train loss 2.093, Learning Rate 1.000e-05, It/sec 1.231, Tokens/sec 134.704, Trained Tokens 68032, Peak mem 4.032 GB\n",
      "Iter 690: Train loss 1.917, Learning Rate 1.000e-05, It/sec 1.338, Tokens/sec 181.628, Trained Tokens 69389, Peak mem 4.741 GB\n",
      "Iter 700: Train loss 2.019, Learning Rate 1.000e-05, It/sec 1.456, Tokens/sec 163.371, Trained Tokens 70511, Peak mem 4.741 GB\n",
      "Iter 700: Saved adapter weights to adapters/adapters.safetensors and adapters/0000700_adapters.safetensors.\n",
      "Iter 710: Train loss 1.819, Learning Rate 1.000e-05, It/sec 1.860, Tokens/sec 168.866, Trained Tokens 71419, Peak mem 4.741 GB\n",
      "Iter 720: Train loss 1.885, Learning Rate 1.000e-05, It/sec 1.662, Tokens/sec 180.161, Trained Tokens 72503, Peak mem 4.741 GB\n",
      "Iter 730: Train loss 1.976, Learning Rate 1.000e-05, It/sec 1.524, Tokens/sec 172.267, Trained Tokens 73633, Peak mem 4.741 GB\n",
      "Iter 740: Train loss 1.726, Learning Rate 1.000e-05, It/sec 1.928, Tokens/sec 173.947, Trained Tokens 74535, Peak mem 4.741 GB\n",
      "Iter 750: Train loss 1.986, Learning Rate 1.000e-05, It/sec 1.618, Tokens/sec 158.097, Trained Tokens 75512, Peak mem 4.741 GB\n",
      "Iter 760: Train loss 1.916, Learning Rate 1.000e-05, It/sec 1.712, Tokens/sec 179.717, Trained Tokens 76562, Peak mem 4.741 GB\n",
      "Iter 770: Train loss 1.851, Learning Rate 1.000e-05, It/sec 1.805, Tokens/sec 180.844, Trained Tokens 77564, Peak mem 4.741 GB\n",
      "Iter 780: Train loss 1.929, Learning Rate 1.000e-05, It/sec 1.637, Tokens/sec 178.385, Trained Tokens 78654, Peak mem 4.741 GB\n",
      "Iter 790: Train loss 1.797, Learning Rate 1.000e-05, It/sec 1.944, Tokens/sec 181.798, Trained Tokens 79589, Peak mem 4.741 GB\n",
      "Iter 800: Val loss 2.231, Val took 7.860s\n",
      "Iter 800: Train loss 1.905, Learning Rate 1.000e-05, It/sec 17.137, Tokens/sec 1873.030, Trained Tokens 80682, Peak mem 4.741 GB\n",
      "Iter 800: Saved adapter weights to adapters/adapters.safetensors and adapters/0000800_adapters.safetensors.\n",
      "Iter 810: Train loss 1.713, Learning Rate 1.000e-05, It/sec 1.776, Tokens/sec 172.451, Trained Tokens 81653, Peak mem 4.741 GB\n",
      "Iter 820: Train loss 1.807, Learning Rate 1.000e-05, It/sec 1.598, Tokens/sec 165.117, Trained Tokens 82686, Peak mem 4.741 GB\n",
      "Iter 830: Train loss 1.805, Learning Rate 1.000e-05, It/sec 1.718, Tokens/sec 164.941, Trained Tokens 83646, Peak mem 4.741 GB\n",
      "Iter 840: Train loss 1.666, Learning Rate 1.000e-05, It/sec 1.920, Tokens/sec 178.942, Trained Tokens 84578, Peak mem 4.741 GB\n",
      "Iter 850: Train loss 1.681, Learning Rate 1.000e-05, It/sec 1.920, Tokens/sec 176.080, Trained Tokens 85495, Peak mem 4.741 GB\n",
      "Iter 860: Train loss 2.206, Learning Rate 1.000e-05, It/sec 1.417, Tokens/sec 170.015, Trained Tokens 86695, Peak mem 4.741 GB\n",
      "Iter 870: Train loss 1.645, Learning Rate 1.000e-05, It/sec 1.910, Tokens/sec 169.448, Trained Tokens 87582, Peak mem 4.741 GB\n",
      "Iter 880: Train loss 1.866, Learning Rate 1.000e-05, It/sec 1.608, Tokens/sec 180.145, Trained Tokens 88702, Peak mem 4.741 GB\n",
      "Iter 890: Train loss 1.839, Learning Rate 1.000e-05, It/sec 1.769, Tokens/sec 180.248, Trained Tokens 89721, Peak mem 4.741 GB\n",
      "Iter 900: Train loss 1.588, Learning Rate 1.000e-05, It/sec 2.035, Tokens/sec 163.601, Trained Tokens 90525, Peak mem 4.741 GB\n",
      "Iter 900: Saved adapter weights to adapters/adapters.safetensors and adapters/0000900_adapters.safetensors.\n",
      "Iter 910: Train loss 1.905, Learning Rate 1.000e-05, It/sec 0.938, Tokens/sec 106.970, Trained Tokens 91666, Peak mem 4.741 GB\n",
      "Iter 920: Train loss 2.034, Learning Rate 1.000e-05, It/sec 1.445, Tokens/sec 172.185, Trained Tokens 92858, Peak mem 4.741 GB\n",
      "Iter 930: Train loss 1.882, Learning Rate 1.000e-05, It/sec 1.183, Tokens/sec 124.735, Trained Tokens 93912, Peak mem 4.741 GB\n",
      "Iter 940: Train loss 1.656, Learning Rate 1.000e-05, It/sec 1.796, Tokens/sec 174.795, Trained Tokens 94885, Peak mem 4.741 GB\n",
      "Iter 950: Train loss 1.795, Learning Rate 1.000e-05, It/sec 1.852, Tokens/sec 174.311, Trained Tokens 95826, Peak mem 4.741 GB\n",
      "Iter 960: Train loss 1.686, Learning Rate 1.000e-05, It/sec 1.963, Tokens/sec 167.833, Trained Tokens 96681, Peak mem 4.741 GB\n",
      "Iter 970: Train loss 1.554, Learning Rate 1.000e-05, It/sec 2.080, Tokens/sec 167.878, Trained Tokens 97488, Peak mem 4.741 GB\n",
      "Iter 980: Train loss 1.929, Learning Rate 1.000e-05, It/sec 1.514, Tokens/sec 167.645, Trained Tokens 98595, Peak mem 4.741 GB\n",
      "Iter 990: Train loss 1.789, Learning Rate 1.000e-05, It/sec 1.809, Tokens/sec 176.732, Trained Tokens 99572, Peak mem 4.741 GB\n",
      "Iter 1000: Val loss 2.274, Val took 7.592s\n",
      "Iter 1000: Train loss 1.581, Learning Rate 1.000e-05, It/sec 13.900, Tokens/sec 1181.485, Trained Tokens 100422, Peak mem 4.741 GB\n",
      "Iter 1000: Saved adapter weights to adapters/adapters.safetensors and adapters/0001000_adapters.safetensors.\n",
      "Saved final adapter weights to adapters/adapters.safetensors.\n"
     ]
    }
   ],
   "source": [
    "# Run QLoRA Training on Quantized Model\n",
    "! mlx_lm.lora --model models/Phi-3.5-mini-instruct-4bit \\\n",
    "    --data data/whatsapp-chats \\\n",
    "    --train \\\n",
    "    --batch-size 2 \\\n",
    "    --lora-layers -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <|user|>\n",
      "You are Devansh Gandhi, a student at the University of Hong Kong. Respond to the following text sent by your friend: 'Wanna eat Indian food tonight?' <thoughts>Your's thoughts</thoughts> tags. Then add your response in a <response>Your response</response> tag. We can only see your answer in the response tag.<|end|>\n",
      "<|assistant|>\n",
      "\n",
      "<thoughts>I bet it's as bad as Chinese food</thoughts>\n",
      "<response>Fine<|end|><|assistant|>\n",
      "==========\n",
      "Prompt: 83 tokens, 340.732 tokens-per-sec\n",
      "Generation: 29 tokens, 48.221 tokens-per-sec\n",
      "Peak memory: 2.154 GB\n"
     ]
    }
   ],
   "source": [
    "! mlx_lm.generate --model models/Phi-3.5-mini-instruct-4bit \\\n",
    "    --adapter-path ./adapters \\\n",
    "    --prompt \"You are Devansh Gandhi, a student at the University of Hong Kong. Respond to the following text sent by your friend: 'Wanna eat Indian food tonight?' <thoughts>Your's thoughts</thoughts> tags. Then add your response in a <response>Your response</response> tag. We can only see your answer in the response tag.\"\\\n",
    "    --max-tokens 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mlx_lm.fuse --model decisionslab/Phi-3.5-mini-instruct-4bit \\\n",
    "    --adapter-path ./adapters \\\n",
    "    --hf-path microsoft/Phi-3.5-mini-instruct \\\n",
    "    --save-path ./models/Phi-3.5-mini-instruct-4-bit-HK \\\n",
    "    --upload-repo decisionslab/Phi-3.5-mini-instruct-4-bit-HK"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
